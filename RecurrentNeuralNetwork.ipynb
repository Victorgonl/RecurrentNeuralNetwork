{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma breve introdução às Redes Neurais Recorrentes (RNN - Recurrent Neural Network)\n",
    "\n",
    "### Universidade Federal de Lavras\n",
    "\n",
    "##### Agosto de 2022\n",
    "\n",
    "* Elaborado por: Victor Gonçalves Lima\n",
    "\n",
    "* Orientado por: Prof. Denilson Alves Pereira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados sequenciais (sequential data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriedades:\n",
    "\n",
    "* A sequência pode conter elementos repetidos\n",
    "\n",
    "* É sensível ao contexto\n",
    "\n",
    "* O tamanhos dos dados varia\n",
    "\n",
    "## Exemplos:\n",
    "\n",
    "* Textos\n",
    "\n",
    "* Audio (voz e música)\n",
    "\n",
    "* Vídeos\n",
    "\n",
    "* Séries de dados\n",
    "\n",
    "* Sequência de DNA\n",
    "\n",
    "## Language models\n",
    "\n",
    "* Generative model\n",
    "\n",
    "* N-grams\n",
    "\n",
    "* Context vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vetorização de contexto (context vectorizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* É um método onde os dados da sequência é dividida em um vetor.\n",
    "\n",
    "<img src=\"figs/Context-vectorizing.png\" width=\"400\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "#### Vantagens:\n",
    "\n",
    "* A ordem é preservada\n",
    "* Funciona para entradas de diferentes tamanhos\n",
    "* Pode ser aplicado ***backpropagation*** e assim aprender\n",
    "* O contexto é presenvado para sentenças curtas ou texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitetura das Redes Neurais Recorrentes (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Por conta de suas propriedades, dados sequenciais não funcionam bem com ***feed-forward networks***, que por definição trabalha com entradas de tamanho fixo.\n",
    "\n",
    "* RNNs são modelos usados para trabalhar com ***dados sequenciais*** incorporando a técnica de ***vetorização de contexto***.\n",
    "\n",
    "* A vetorização permite que as RNNs busque informações passadas já calculadas ao longo da sequencia, assim, as RNNs permitem a entrada de múltiplos vetores, produzindo uma mais mais saídas vetoriais.\n",
    "\n",
    "* De uma forma simplificada, RNN são redes neurais com loops, permitindo que as informações anteriores sejam preservadas.\n",
    "\n",
    "<img src=\"figs/RNN-rolled.png\" width=\"150\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>\n",
    "\n",
    "* Desenrolando este loop, verificamos que a RNN se apresenta como múltiplas cópias da mesma rede, cada uma passando a informação para um sucessor.\n",
    "\n",
    "<img src=\"figs/RNN-unrolled.png\" width=\"800\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time step *t*\n",
    "* Nas RNN, *x(t)* é uma entrada na rede em um paço de tempo *t*, utilizado para indicar a ordem que uma palavra, por exemplo, ocorre na sequência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden state *h(t)*\n",
    "\n",
    "* Um *h(t)* representa um ***vetor contextual*** em um tempo *t* e age como uma \"mémoria\" da rede.\n",
    "\n",
    "* O ***vetor contextual*** *h(t)* é calculado a partir da entrada atual *x(t)* e da entrada em uma unidade de tempo *t* anterior da ***hidden state*** *h(t-1)*.\n",
    "\n",
    "* $h_{t} = tanh(W_{h}h_{t-1} + W_{x}x_{t})$\n",
    "\n",
    "#### Compartilhamento de parâmetros (parameter sharing)\n",
    "\n",
    "* Os parametros $\\{W_{h}, W_{x}, W_{y}\\}$ são constantes para todas as entradas da rede, permitindo que as RNN mantenham a informação contextual mesmo com variações no tamanho da sentança de entrada.\n",
    "\n",
    "<img src=\"figs/Recurrent-neural-network.png\" width=\"200\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "* RNNs compartilham os mesmos parâmetros por várias unidades de tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predição\n",
    "\n",
    "* A predição de uma RNN será a saída do último ***hiden state*** junto ao parâmetro de saída $W_{y}$\n",
    "\n",
    "* Tal predição é similar a um ***problema de classificação***, e então uma função ***softmax*** é aplicada a fim de prever a saída ideal de acordo com as possiblidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computando os gradientes\n",
    "\n",
    "* Durante o ***backpropagation***, a rede deve voltar cada paço de tempo para atualizar os parâmetros.\n",
    "\n",
    "* Como prever a saída de uma RNN é um ***problema de classificação***, é utilizado ***cross-entropy*** para calcular a perda.\n",
    "\n",
    "* **Cross-entropy:** $L_{θ}(y,y’)_{t} = -y+{t}\\log{y_{t}'}$, onde $θ = \\{Wh,Wx,Wy\\}$.\n",
    "\n",
    "## Backpropagation through time (BPTT)\n",
    "\n",
    "* A rede precisa ser desenrolada para que seus parâmetros sejam diferenciados pela rede respeitando o ***time step***.\n",
    "\n",
    "* Como a rede pega uma palavra por vez, o cálculo da perda é baseado por cada palavra.\n",
    "\n",
    "<img src=\"figs/Backpropagation.png\" width=\"400\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemas com as RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As aplicações de RNN utilizam o contexto (entradas anteriores $x_{0}$, $x_{1}$, ...) para executar sua tarefa.\n",
    "\n",
    "<img src=\"figs/RNN-shorttermdepdencies.png\" width=\"500\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>\n",
    "\n",
    "* Quanto maior a distância entre o contexto ($x_{0}$, $x_{1}$, ...) e a saída ($h_{t+1}$), mais difícil será para a RNN conectar a informação necessária.\n",
    "\n",
    "<img src=\"figs/RNN-longtermdependencies.png\" width=\"600\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas com gradientes\n",
    "\n",
    "###  Vanishing gradients\n",
    "\n",
    "* Cada unidade de neurônio na rede participa do cálculo da saída, assim todos devem ter seus parâmetros atualizados a fim de minimizar os erros, voltando em todos os ***time step**.\n",
    "\n",
    "* O vetor contextual e os parâmetros da hiden state é compartilhado por toda rede a fim de preservar a continuidade da sequência.\n",
    "\n",
    "* No inicio do treinamento, o parâmetro é inicializado com um número aleatório próximo a zero.\n",
    "\n",
    "* Com o ***hiden state***  sendo multiplicado por si próprio enquanto avança o ***time step***, o gradiente $W_{h}$ se torna tão pequeno e tende a zero.\n",
    "\n",
    "* Quando menor o gradiente, mais difpicil será para a rede atualizar seus pesos, e se o gradiente for zero, os pesos para de ser atualizados.\n",
    "\n",
    "* Com o gradiente tendendo a zero de forma exponencialmente rápida, faz com que a rede tenha dificuldades em aprender com informações muito distantes.\n",
    "\n",
    "### Exploding gradients\n",
    "\n",
    "* Ocore no processo de ***backpropagation*** quando gradientes altos são acumulados devido a um processo instável, resultando em atualizações muito grandes nos parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* São criadas na intenção de preservar informações contextuais de longa distância, perdidas em RNN tradicionais devido aos ***problemas com gradientes***.\n",
    "\n",
    "* Ao envés de terem apenas uma camada de rede neural, LSTMs possuem quatro, interagindo de forma que preservem contextos distantes.\n",
    "\n",
    "<img src=\"figs/Long-Short-Term-Memory.png\" width=\"500\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "## Funcionamento da LSTM\n",
    "\n",
    "##### ***Cell state:***\n",
    "\n",
    "* É a parte ***long-term memory***, denotada por $C_{t-1}$ e recursiva.\n",
    "\n",
    "* Permite que informações anteriores sejam armazenadas na rede LSTM.\n",
    "\n",
    "* É modulada internamente por *gates* denominados ***forget gate*** e ***input gate***.\n",
    "\n",
    "<img src=\"figs/LSTM-network-1.png\" width=\"400\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "##### ***Forget Gate:***\n",
    "\n",
    "* É responsável por decidir qual informação será preservada e qual será esquecida.\n",
    "\n",
    "* Seu funcionamento é através da ***função sigmoid***.\n",
    "\n",
    "* Olha o estado inicial em $h_{t-1}$ e $x_{t}$ e retorna um número entre 0 e 1 para cada ***cell state*** $C_{t-1}$.\n",
    "\n",
    "* Se a informação retornada for 1, ele guarda a informação, e se retornar 0, ele apaga.\n",
    "\n",
    "<img src=\"figs/LSTM-network-2.png\" width=\"500\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "##### ***Input gate:***\n",
    "\n",
    "* Pega a mesma entrada $x_{t} do estado inicial $h_{t-1}$.\n",
    "\n",
    "* Passa por duas ***funções de ativição*** não lineares: ***sigmoid*** e ***tanh***.\n",
    "\n",
    "* O retorno se torna candidato para $C_{t}$.\n",
    "\n",
    "* Assim, ajuda o ***cell state** salvar novas informações e atuaizar de $C_{t-1}$ à $C_{t}$.\n",
    "\n",
    "<img src=\"figs/LSTM-network-3.png\" width=\"500\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "##### ***Output gate:***\n",
    "\n",
    "* É baseado no ***cell state*** $C_{t}$, o ***hiden state*** inicial $h_{t-1}$ e a entrada $x_{t}$.\n",
    "\n",
    "* $h_{t-1}$ e $x_{t}$ são passados pela ***sigmoid***, com retorno entre 0 e 1.\n",
    "\n",
    "* Simultaneamente, $C_{t} é passado pela ***tanh***, com retorno entre -1 e 1.\n",
    "\n",
    "* A informação passada para o próximo ***hiden state*** é a multiplicação dos retornos das ***funções de ativação*** calculadas.\n",
    "\n",
    "<img src=\"figs/LSTM-network-4.png\" width=\"500\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
