{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma breve introdução às Redes Neurais Recorrentes (RNN - Recurrent Neural Network)\n",
    "\n",
    "### Universidade Federal de Lavras\n",
    "\n",
    "##### Agosto de 2022\n",
    "\n",
    "* Elaborado por: Victor Gonçalves Lima\n",
    "\n",
    "* Orientado por: Prof. Denilson Alves Pereira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados sequenciais (sequential data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriedades:\n",
    "\n",
    "* A sequência pode conter elementos repetidos\n",
    "\n",
    "* É sensível ao contexto\n",
    "\n",
    "* O tamanho dos dados varia\n",
    "\n",
    "## Exemplos:\n",
    "\n",
    "* Textos\n",
    "\n",
    "* Audio (voz e música)\n",
    "\n",
    "* Vídeos\n",
    "\n",
    "* Séries de dados\n",
    "\n",
    "* Sequência de DNA\n",
    "\n",
    "## Language models\n",
    "\n",
    "* Generative model\n",
    "\n",
    "* N-grams\n",
    "\n",
    "* Context vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vetorização de contexto (context vectorizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* É um método onde os dados da sequência é dividida em um vetor.\n",
    "\n",
    "<img src=\"figs/Context-vectorizing.webp\" width=\"500\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "#### Vantagens:\n",
    "\n",
    "* A ordem é preservada\n",
    "* Funciona para entradas de diferentes tamanhos\n",
    "* Pode ser aplicado ***backpropagation*** e assim aprender\n",
    "* O contexto é presenvado para sentenças curtas ou texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitetura das Redes Neurais Recorrentes (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Por conta de suas propriedades, dados sequenciais não funcionam bem com ***feed-forward networks***, que por definição trabalha com entradas de tamanho fixo.\n",
    "\n",
    "* RNNs são modelos usados para trabalhar com ***dados sequenciais*** incorporando a técnica de ***vetorização de contexto***.\n",
    "\n",
    "* A vetorização permite que as RNNs busque informações passadas já calculadas ao longo da sequencia, assim, as RNNs permitem a entrada de múltiplos vetores, produzindo uma mais mais saídas vetoriais.\n",
    "\n",
    "* De uma forma simplificada, RNN são redes neurais com loops, permitindo que as informações anteriores sejam preservadas.\n",
    "\n",
    "<img src=\"figs/RNN-rolled.png\" width=\"150\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>\n",
    "\n",
    "* Desenrolando este loop, verificamos que a RNN se apresenta como múltiplas cópias da mesma rede, cada uma passando a informação para um sucessor.\n",
    "\n",
    "<img src=\"figs/RNN-unrolled.png\" width=\"800\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time step *t*\n",
    "* Nas RNN, *x(t)* é uma entrada na rede em um paço de tempo *t*, utilizado para indicar a ordem que uma palavra, por exemplo, ocorre na sequência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden state *h(t)*\n",
    "\n",
    "* Um *h(t)* representa um ***vetor contextual*** em um tempo *t* e age como uma \"mémoria\" da rede.\n",
    "\n",
    "* O ***vetor contextual*** *h(t)* é calculado a partir da entrada atual *x(t)* e da entrada em uma unidade de tempo *t* anterior da ***hidden state*** *h(t-1)*.\n",
    "\n",
    "* $h_{t} = tanh(W_{h}h_{t-1} + W_{x}x_{t})$\n",
    "\n",
    "#### Compartilhamento de parâmetros (parameter sharing)\n",
    "\n",
    "* Os parametros $\\{W_{h}, W_{x}, W_{y}\\}$ são constantes para todas as entradas da rede, permitindo que as RNN mantenham a informação contextual mesmo com variações no tamanho da sentança de entrada.\n",
    "\n",
    "<img src=\"figs/Recurrent-neural-network.png\" width=\"200\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "* RNNs compartilham os mesmos parâmetros por várias unidades de tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predição\n",
    "\n",
    "* A predição de uma RNN será a saída do último ***hiden state*** junto ao parâmetro de saída $W_{y}$\n",
    "\n",
    "* Tal predição é similar a um ***problema de classificação***, e então uma função ***softmax*** é aplicada a fim de prever a saída ideal de acordo com as possiblidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computando os gradientes\n",
    "\n",
    "* Durante o ***backpropagation***, a rede deve voltar cada paço de tempo para atualizar os parâmetros.\n",
    "\n",
    "* Como prever a saída de uma RNN é um ***problema de classificação***, é utilizado ***cross-entropy*** para calcular a perda.\n",
    "\n",
    "* **Cross-entropy:** $L_{θ}(y,y’)_{t} = -y+{t}\\log{y_{t}'}$, onde $θ = \\{Wh,Wx,Wy\\}$.\n",
    "\n",
    "## Backpropagation through time (BPTT)\n",
    "\n",
    "* A rede precisa ser desenrolada para que seus parâmetros sejam diferenciados pela rede respeitando o ***time step***.\n",
    "\n",
    "* Como a rede pega uma palavra por vez, o cálculo da perda é baseado por cada palavra.\n",
    "\n",
    "<img src=\"figs/Backpropagation.webp\" width=\"400\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemas com as RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As aplicações de RNN utilizam o contexto (entradas anteriores $x_{0}$, $x_{1}$, ...) para executar sua tarefa.\n",
    "\n",
    "<img src=\"figs/RNN-shorttermdepdencies.png\" width=\"600\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>\n",
    "\n",
    "* Quanto maior a distância entre o contexto ($x_{0}$, $x_{1}$, ...) e a saída ($h_{t+1}$), mais difícil será para a RNN conectar a informação necessária.\n",
    "\n",
    "<img src=\"figs/RNN-longtermdependencies.png\" width=\"600\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas com gradientes\n",
    "\n",
    "###  Vanishing gradients\n",
    "\n",
    "* Cada unidade de neurônio na rede participa do cálculo da saída, assim todos devem ter seus parâmetros atualizados a fim de minimizar os erros, voltando em todos os ***time step**.\n",
    "\n",
    "* O vetor contextual e os parâmetros da hiden state é compartilhado por toda rede a fim de preservar a continuidade da sequência.\n",
    "\n",
    "* No inicio do treinamento, o parâmetro é inicializado com um número aleatório próximo a zero.\n",
    "\n",
    "* Com o ***hiden state***  sendo multiplicado por si próprio enquanto avança o ***time step***, o gradiente $W_{h}$ se torna tão pequeno e tende a zero.\n",
    "\n",
    "* Quando menor o gradiente, mais difpicil será para a rede atualizar seus pesos, e se o gradiente for zero, os pesos para de ser atualizados.\n",
    "\n",
    "* Com o gradiente tendendo a zero de forma exponencialmente rápida, faz com que a rede tenha dificuldades em aprender com informações muito distantes.\n",
    "\n",
    "### Exploding gradients\n",
    "\n",
    "* Ocore no processo de ***backpropagation*** quando gradientes altos são acumulados devido a um processo instável, resultando em atualizações muito grandes nos parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alguns tipos de RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bidirectional RNN\n",
    "\n",
    "<img src=\"figs/RNN-bidirectional.png\" width=\"400\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "* Encoder-decoder sequence-to-sequence architecture\n",
    "\n",
    "<img src=\"figs/Encoder-decoder.webp\" width=\"400\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "* Convolutional Recurrent Neural Network (CRNN)\n",
    "\n",
    "<img src=\"figs/Convolutional-recurrent-neural-network.webp\" width=\"400\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "* Long Short Term Memory (LSTM)\n",
    "\n",
    "<img src=\"figs/Long-Short-Term-Memory.webp\" width=\"400\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>\n",
    "\n",
    "* Gated Recurrent Unit (GRU)\n",
    "\n",
    "<img src=\"figs/Gated-Recurrent-Unit.webp\" width=\"400\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* São criadas na intenção de preservar informações contextuais de longa distância, perdidas em RNN tradicionais devido aos ***problemas com gradientes***.\n",
    "\n",
    "* Ao envés de terem apenas uma camada de rede neural, LSTMs possuem quatro, interagindo de forma que preservem contextos distantes.\n",
    "\n",
    "<img src=\"figs/LSTM3-chain.png\" width=\"700\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcionamento da LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/LSTM-arch.webp\" width=\"700\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Cell state:***\n",
    "\n",
    "* É a parte ***long-term memory***, denotada por $C_{t-1}$ e recursiva.\n",
    "\n",
    "* Permite que informações anteriores sejam armazenadas na rede LSTM.\n",
    "\n",
    "* É modulada internamente por *gates* denominados ***forget gate*** e ***input gate***.\n",
    "\n",
    "<img src=\"figs/LSTM3-C-line.png\" width=\"700\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Forget Gate:***\n",
    "\n",
    "* É responsável por decidir qual informação será preservada e qual será esquecida.\n",
    "\n",
    "* Seu funcionamento é através da ***função sigmoid***.\n",
    "\n",
    "* Olha o estado inicial em $h_{t-1}$ e $x_{t}$ e retorna um número entre 0 e 1 para cada ***cell state*** $C_{t-1}$.\n",
    "\n",
    "* Se a informação retornada for 1, ele guarda a informação, e se retornar 0, ele apaga.\n",
    "\n",
    "<img src=\"figs/LSTM3-focus-f.png\" width=\"700\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Input gate:***\n",
    "\n",
    "* Pega a mesma entrada $x_{t} do estado inicial $h_{t-1}$.\n",
    "\n",
    "* Passa por duas ***funções de ativação***: ***sigmoid*** e ***tanh***.\n",
    "\n",
    "* O retorno se torna candidato para $C_{t}$.\n",
    "\n",
    "<img src=\"figs/LSTM3-focus-i.png\" width=\"700\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>\n",
    "\n",
    "* Assim, ajuda o ***cell state*** salvar novas informações e atuaizar $C_{t-1}$ à $C_{t}$.\n",
    "\n",
    "<img src=\"figs/LSTM3-focus-C.png\" width=\"700\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Output gate:***\n",
    "\n",
    "* É baseado no ***cell state*** $C_{t}$, o ***hiden state*** inicial $h_{t-1}$ e a entrada $x_{t}$.\n",
    "\n",
    "* $h_{t-1}$ e $x_{t}$ são passados pela ***sigmoid***, com retorno entre 0 e 1.\n",
    "\n",
    "* Simultaneamente, $C_{t}$ é passado pela ***tanh***, com retorno entre -1 e 1.\n",
    "\n",
    "* A informação passada para o próximo ***hiden state*** é a multiplicação dos retornos das ***funções de ativação*** calculadas.\n",
    "\n",
    "<img src=\"figs/LSTM3-focus-o.png\" width=\"700\" title=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM (Bi-LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Diferente das ***LSTMs*** unidirecionais, permite obter informações sequenciais em ambas direções (passado e futuro do ***time step***).\n",
    "\n",
    "<img src=\"figs/Bi-LSTM.jpg\" width=\"800\" title=\"https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* É uma versão modificada da ***LSTM*** com menor complexidade.\n",
    "\n",
    "* Utilizam ***hiden state*** no lugar de ***cell state*** para tranferir informações.\n",
    "\n",
    "* Possuem dois ***gates***: ***reset gate*** e ***update gate***.\n",
    "\n",
    "* São mais rápidos que as ***LSTM***.\n",
    "\n",
    "<img src=\"figs/Gated-Recurrent-Unit.webp\" width=\"600\" title=\"https://neptune.ai/blog/recurrent-neural-network-guide\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Update gate***:\n",
    "\n",
    "* Seleciona as informações que precisam ser adicionadas e/ou ignoradas\n",
    "\n",
    "##### ***Reset gate***:\n",
    "\n",
    "* Decide o quanto de informações anteriores serão esquecidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next word prediction using Bi-LSTM\n",
    "\n",
    "Adaptado de: https://www.kaggle.com/code/ysthehurricane/next-word-prediction-bi-lstm-tutorial-easy-way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset e pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* O dataset contèm informações de artigos escolhidos de forma aleatória publicados em 2019.\n",
    "\n",
    "* Os dados utilizados para treinamento serão os títulos desses artigos.\n",
    "\n",
    "* Fonte: https://www.kaggle.com/datasets/dorianlazar/medium-articles-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_data = pd.read_csv('./medium-articles-dataset/medium_data.csv')\n",
    "medium_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de artigos: \", medium_data.shape[0])\n",
    "print(\"Número de atributos: \", medium_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_data['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removendo caracteres e palavras indesejadas nos títulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_data['title'] = medium_data['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\n",
    "medium_data['title'] = medium_data['title'].apply(lambda x: x.replace('\\u200a',' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenzation\n",
    "\n",
    "* É o processo de indexar todas as palavras com uma identificação numérica única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<oov>')\n",
    "tokenizer.fit_on_texts(medium_data['title'])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número total de palavras: \", total_words)\n",
    "print(tabulate([['Artificial', tokenizer.word_index['artificial']],\n",
    "                ['Neural', tokenizer.word_index['neural']],\n",
    "                ['Network', tokenizer.word_index['network']]], headers=['Word', 'ID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformar textos em sequências e gerar modelos ***n-grams***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in medium_data['title']:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de sequências de entrada: \", len(input_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padronizar sequências para terem o mesmo tamanho usando ***padding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotular e preparar dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição e compilação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(xs, ys, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatísticas do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***accuracy:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***loss:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./medium-articles-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./medium-articles-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predizendo a próxima palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = input()\n",
    "predicted_sentence = input_sentence\n",
    "\n",
    "NEXT_WORDS = 2\n",
    "\n",
    "for _ in range(NEXT_WORDS):\n",
    "    token_list = tokenizer.texts_to_sequences([predicted_sentence])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = tokenizer.index_word[np.argmax(model.predict(token_list, verbose=0))]\n",
    "    predicted_sentence += \" \" + predicted\n",
    "\n",
    "output = \"Entrada:\\t\" + input_sentence\n",
    "for _ in range(NEXT_WORDS):\n",
    "    output += \" ___\"\n",
    "print(output)\n",
    "print(\"Predição:\\t\" + predicted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências\n",
    "\n",
    "- https://neptune.ai/blog/recurrent-neural-network-guide\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2021/07/in-depth-explanation-of-recurrent-neural-network/\n",
    "\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "- http://colah.github.io/posts/2015-09-NN-Types-FP/\n",
    "\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "- https://analyticsindiamag.com/lstm-vs-gru-in-recurrent-neural-network-a-comparative-study/\n",
    "\n",
    "- https://www.tensorflow.org/guide/keras/rnn\n",
    "\n",
    "- https://www.kaggle.com/code/ysthehurricane/next-word-prediction-bi-lstm-tutorial-easy-way\n",
    "\n",
    "- https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
